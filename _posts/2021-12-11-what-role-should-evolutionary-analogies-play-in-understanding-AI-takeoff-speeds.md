---
layout: post
title:  "What role should evolutionary analogies play in understanding AI takeoff speeds?"
date:   2021-12-11
published: true
excerpt_separator: <!--more-->
---

<!--more-->

I recently wrote a post/report based on my recent AI safety research internship, with Dr. Vael Gates at Stanford. 

A common concern within AI safety is that the development and influence of AI is going to be very fast. This could cause problems such as sudden massive unemployment and the potential for mass misuse (since the technologies are changing faster than legal systems and governments are able to adapt). There are also concerns that really powerful AI systems could result in "transformative" changes to society as large as the industrial revolution, but happening over the course of *a few years* - a situation that we are currently unprepared for.

But why do people believe that such a situation is likely? One common analogy to the increasing power of AI systems is based on the sudden increase in intelligent capabilities in evolution, arguably leading to the rise of human civilisation. The goal of this report is to analyse analogies (and disanalogies) like these, and see what the implications might be for future AI safety research. 

You can read the report on the [Effective Altruism forum](https://forum.effectivealtruism.org/posts/aSDnzAm85a3Pi87rm/what-role-should-evolutionary-analogies-play-in) or [LessWrong](https://www.lesswrong.com/posts/teD4xjwoeWc4LyRAD/what-role-should-evolutionary-analogies-play-in). 