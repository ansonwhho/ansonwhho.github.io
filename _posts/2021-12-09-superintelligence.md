---
layout: post
title:  "[Summary] Superintelligence: Paths, Dangers, Strategies"
date:   2021-12-09
published: true
excerpt_separator: <!--more-->
---

<!--more-->

 **Epistemic status**: moderate confidence, likely a few mistakes or misconceptions. Note that these are meant to be my personal notes based on self-study - if you're seeking informed expert opinion on AI safety, you should probably look elsewhere.

 This is a summary and review of *[Superintelligence: Paths, Dangers, Strategies](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)* by [Nick Bostrom](https://www.nickbostrom.com/). 

 <br />

## Overview
I bought this book at the annual [Hong Kong Book Fair](https://en.wikipedia.org/wiki/Hong_Kong_Book_Fair) several years before I'd even heard about AI Safety. I doubt that I had the faintest idea what the book was about at the time, so I have no idea why I bought it. 

A lot of what I've heard about it is the "[paperclip maximiser](https://www.lesswrong.com/tag/paperclip-maximizer)" thought experiment, and its overall Sci-Fiesque and abstract philosophical disposition. This usually turns people away from the book, and I know a couple of friends who gave up reading it. 

For some reason I've had a bit of trouble finding a good summary of the book's high-level points, so I thought I'd have a go at it myself. I think I should point out that this is *my take* on the book, and it's likely that I've misrepresented some of Bostrom's views.  

 ## Summary
Artificial Intelligence (AI) has been growing increasingly powerful, and since around the 1950s, some AI researchers have been working towards the goal of building Artificial General Intelligence (AGI). This is a system that is able to exhibit intelligent behaviour not just in a single domain (e.g. chess), but on multiple different tasks. 

If we succeed at this, then a natural question we might ask is, "what if we build an AGI that is smarter than us, i.e. *superintelligent*?". In this book, Bostrom argues that there's been shockingly little consideration of the risks associated with this. In particular, he raises concerns about the risk of having the future being controlled by a superintelligent AI, should it arise. 

This all seems terribly wacky, so how on Earth is Bostrom going to justify this? In my opinion, the book is roughly split into three main parts: 
1. The plausibility of superintelligence
2. A world with superintelligent AI
3. What we should do now

 ### A look into history
The history of *Homo sapiens* has gone through a series of sharp growth modes, like the [Agricultural](https://en.wikipedia.org/wiki/Agricultural_revolution) and [Industrial Revolutions](https://en.wikipedia.org/wiki/Industrial_Revolution). 

In fact, we're currently at a time of very fast economic growth, especially when compared to the [preceding thousands of years](https://en.wikipedia.org/wiki/Human_history)[^1]. But what if the world underwent another jump in growth, due to AI? As we'll see, there are some pretty compelling arguments for why we ought to at least *think* about such a scenario. 



The first objection one might raise is that researchers have often [overstated the capabilities of AI in the past](https://en.wikipedia.org/wiki/AI_winter). But there have also been times where we significantly *underestimated* AI - some 
(In fact, Bostrom mentions that trends would suggest world-champion level Go playing AI would arrive by around 2024. This feat was achieved by AlphaGo in 2016, and AlphaGo has since been thoroughly demolished by even more powerful systems.)

## Thoughts
I think I can definitely understand why so many people were put off by this book. I doubt I would've been able to make it through the first chapter if I hadn't already engaged quite a bit with AI safety before. 

A lot of the arguments are written at quite a high-level, and current (Dec 2021) understanding of the views is quite different. 

There are probably a lot of ways in which we could disagree with the book, with claims like: 
* We'll never reach superintelligence, or at least not for centuries
* People have been overoptimistic about the capabilities of AI in the past
* People will recognise these dangers once they arise, and respond to them

Given how often these objections are raised, I'll leave this topic to another post. 
---

[^1]: This is exemplified by the famous ["hockey stick curve"](https://www.core-econ.org/the-economy/book/text/01.html) that quite aptly describes several measures of economic growth. 