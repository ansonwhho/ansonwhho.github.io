---
layout: post
title:  Seeking Survey Responses - Attitudes Towards AI Risks
date:   2022-03-28
published: false
excerpt_separator: <!--more-->
tags: EA AI-safety
author: Anson Ho
---
> Conducting a survey about attitudes towards AI risks, to see how to present arguments about AI safety

<!--more-->

**TL;DR we’re conducting a survey about attitudes towards AI risks, targeted towards people in the EA and rationality communities. We're interested in responses whether or not you're involved in AI safety.** [**Google Forms Link here**](https://forms.gle/QTm87WWMKo6ovUYD8)**.**

Esben Kran and I previously published an older version of this survey which we've edited based on feedback. **If you’ve completed a previous version of this survey, you don’t need to fill it out again.** See also [Esben's question on LessWrong](https://www.lesswrong.com/posts/7ys4kmXKaJKdGC69B/your-specific-attitudes-towards-ai-safety). 

Motivation
==========

*   Recently, there has been some discussion about [how](https://forum.effectivealtruism.org/posts/zsFCj2mfnYZmSW2FF/ai-risk-is-like-terminator-stop-saying-it-s-not-1) [to](https://forum.effectivealtruism.org/posts/GvHPnzGJQJ7iAiJNr/on-presenting-the-case-for-ai-risk) [present](https://forum.effectivealtruism.org/posts/rFpfW2ndHSX7ERWLH/simplify-ea-pitches-to-holy-shit-x-risk) arguments about AI safety and existential risks more generally
*   One disagreement is about how receptive people are to existing arguments, which may depend on personal knowledge/background, how the arguments are presented, etc.
*   We hope to take first steps towards a more empirical approach, by first gathering information about existing opinions and using this to inform outreach
    *   While [other](https://arxiv.org/abs/2105.02117) [surveys](https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios) [exist](https://arxiv.org/abs/1705.08807), our survey focuses more on perceptions within the EA and rationality communities (not just on researchers), and on AI risk *arguments* in particular
    *   We also think of this as a cheap test for similar projects in the future

The Survey
==========

*   We expect this to take 5-10 min to complete, and hope to receive around 100 responses in total
*   [Link to the survey](https://forms.gle/QTm87WWMKo6ovUYD8)
*   We're hoping to receive responses whether or not you're interested in AI safety

Expected Output
===============

*   Through the survey, we hope to:
    *   Get a better understanding of how personal background and particular arguments contribute to perception of AI safety as a field, and to use this as a rough guide for AI safety outreach
    *   Test the feasibility of similar projects
*   We intend on publishing the results and our analysis on LessWrong and the EA forum
*   Note that this is still quite experimental - we welcome questions and feedback!
    *   While we have done some user tests for the survey, we fully expect there to be things that we missed or are ambiguous
    *   If you’ve already filled out the survey or given us feedback, thank you!